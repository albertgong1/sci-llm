
"""Generate Harbor tasks for the SuperCon precedent search (dev set)."""

import argparse
import csv
import json
import shutil
from pathlib import Path

def templates_dir() -> Path:
    return Path("search-template")

def pbench_eval_dir() -> Path:
    return Path("../../src/pbench_eval")

def read_template(relative_path: str) -> str:
    return (templates_dir() / relative_path).read_text()

def copy_template(relative_path: str, dest_path: Path) -> None:
    dest_path.parent.mkdir(parents=True, exist_ok=True)
    shutil.copy2(templates_dir() / relative_path, dest_path)

def dockerfile_contents() -> str:
    return read_template("environment/Dockerfile")

def slugify(value: str) -> str:
    return (
        value.lower()
        .replace(" ", "-")
        .replace("/", "-")
        .replace("(", "")
        .replace(")", "")
    )

def build_task(task_dir: Path, row: dict[str, str], task_name: str) -> None:
    # 1. Setup dirs
    env_dir = task_dir / "environment"
    tests_dir = task_dir / "tests"
    solution_dir = task_dir / "solution"
    
    for d in [env_dir, tests_dir, solution_dir]:
        d.mkdir(parents=True, exist_ok=True)

    # 2. Extract values
    material = row["material"]
    is_sc = row["Has material been reported to be superconducting?"]  # "Yes" or "No"
    
    # Handle potentially empty/float values in CSV
    # The CSV was generated by pandas, so empty might be empty string or not present
    highest_tc = str(row.get("What is the highest measured Tc?", "") or "")
    lowest_tcn = str(row.get("What is the lowest temp for measurement at which material was not superconducting?", "") or "")

    # Cleanup values
    # Cleanup values: Ensure everything is N/A if missing
    if not highest_tc or highest_tc.lower() == "nan": 
        highest_tc = "N/A"
    if not lowest_tcn or lowest_tcn.lower() == "nan": 
        lowest_tcn = "N/A"

    # 3. Create Expected JSON (Ground Truth)
    # Mapping to the 3 output properties
    expected_rows = [
        {
            "material": material,
            "property_name": "is_superconducting",
            "property_value": is_sc,
            "rubric": "categorical"
        },
        {
            "material": material,
            "property_name": "tc",
            "property_value": highest_tc,
            "rubric": "categorical" if highest_tc == "N/A" else "0.1% SI"
        },
        {
            "material": material,
            "property_name": "tcn",
            "property_value": lowest_tcn,
            "rubric": "categorical" if lowest_tcn == "N/A" else "0.1% SI"
        }
    ]
    
    expected = {
        "task": task_name,
        "refno": "precedent-search", # Placeholder
        "ground_truth": expected_rows,
    }
    (tests_dir / "expected.json").write_text(json.dumps(expected, indent=2))
    
    # 4. Instruction
    instruction_template = read_template("instruction.md.template")
    # Simple substitution
    instruction = instruction_template.replace("{material}", material)
    (task_dir / "instruction.md").write_text(instruction)
    
    # 5. Task TOML
    task_toml = read_template("task.toml.template").replace("{task_name}", task_name)
    (task_dir / "task.toml").write_text(task_toml)
    
    # 6. Dockerfile and Test Scripts
    (env_dir / "Dockerfile").write_text(dockerfile_contents())
    copy_template("tests/check_prediction.py", tests_dir / "check_prediction.py")
    copy_template("tests/test.sh", tests_dir / "test.sh")

    # Copy shared scoring utils
    utils_path = tests_dir / "pbench_eval_utils.py"
    shutil.copy2(pbench_eval_dir() / "utils.py", utils_path)
    
    # PATCH: Remove space_groups_normalized.json dependency for this specific task
    # because we only need scorer_si and scorer_categorical
    content = utils_path.read_text()
    # Logic to identify and remove the loading block
    # We look for the block starting with "ASSETS_DIR =" and ending with "json.load(f)"
    # We'll just regex replace it or string replace safely
    import re
    # Pattern matches the import block down to the json load
    pattern = r'ASSETS_DIR = Path\(__file__\)\.parent\.parent\.parent / "assets".*?SPACE_GROUPS = json\.load\(f\)'
    # Replace with empty dict
    patched_content = re.sub(pattern, 'SPACE_GROUPS = {}', content, flags=re.DOTALL)
    utils_path.write_text(patched_content)
    
    # 7. Oracle Solution (solve.sh)
    # We construct a prediction matching the expected rows exactly
    prediction_rows = []
    for er in expected_rows:
        prediction_rows.append({
            "material": er["material"],
            "property_name": er["property_name"],
            "value_string": er["property_value"] 
        })
        
    solution_json = { "properties": prediction_rows }
    
    solution_script = f"""#!/bin/bash
set -euo pipefail

mkdir -p /app/output
cat > /app/output/predictions.json <<'EOF'
{json.dumps(solution_json, indent=2)}
EOF
"""
    (solution_dir / "solve.sh").write_text(solution_script)
    (solution_dir / "solve.sh").chmod(0o755)
    (tests_dir / "test.sh").chmod(0o755)


def write_job_config(tasks_dir: Path, job_path: Path) -> None:
    # We assume we are running from examples/tc-precedent-search/
    # So repo root is ../../
    # We still need repo_root here because 'job.yaml' is used by the harbor runner
    # which is executed from the repository root. Thus, the paths in job.yaml
    # must be relative to the repository root, not this script.
    repo_root = Path("../..").resolve()
    
    workspace_root = (repo_root / "examples/harbor-workspace").resolve()
    
    if not tasks_dir.is_absolute():
        tasks_full = (Path.cwd() / tasks_dir).resolve()
    else:
        tasks_full = tasks_dir.resolve()
        
    try:
        tasks_rel = tasks_full.relative_to(workspace_root)
    except ValueError:
        # Fallback if somehow tasks are outside workspace (e.g. absolute path usage)
        # But for this script's purpose, it should be inside.
        print(f"Warning: Tasks dir {tasks_full} is not within workspace {workspace_root}. Using repo-relative path.")
        tasks_rel = tasks_full.relative_to(repo_root)
    job_yaml = f"""\
jobs_dir: jobs
n_attempts: 3
timeout_multiplier: 1.0
orchestrator:
  type: local
  n_concurrent_trials: 85
  quiet: false
environment:
  type: docker
  force_build: true
  delete: true
agents:
  - name: oracle
datasets:
  - path: {tasks_rel.as_posix()}
"""
    job_path.parent.mkdir(parents=True, exist_ok=True)
    job_path.write_text(job_yaml)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--csv", type=Path, default=Path("SuperCon_Tc_Tcn_dev-set.csv"))
    parser.add_argument("--output-dir", type=Path, default=Path("../../examples/harbor-workspace/out/harbor/precedent-search"))
    parser.add_argument("--limit", type=int, default=None)
    parser.add_argument("--write-job-config", action="store_true")
    parser.add_argument("--force", action="store_true")
    
    args = parser.parse_args()
    
    task_root = args.output_dir / "tc-precedent-search"
    tasks_dir = task_root / "tasks"
    
    if task_root.exists():
        if args.force:
            shutil.rmtree(task_root)
        elif any(task_root.iterdir()):
            print(f"Directory {task_root} exists. Use --force to overwrite.")
            return

    tasks_dir.mkdir(parents=True, exist_ok=True)

    print(f"Reading CSV: {args.csv}")
    with args.csv.open() as f:
        reader = csv.DictReader(f)
        rows = list(reader)
        
    if args.limit:
        rows = rows[:args.limit]
        
    print(f"Generating tasks for {len(rows)} materials...")
    
    for row in rows:
        material = row["material"]
        task_id = slugify(material)
        task_dir = tasks_dir / task_id
        task_dir.mkdir(parents=True, exist_ok=True)
        
        build_task(task_dir, row, "precedent-search")
        # print(f"Built task: {task_id}")
        
    print(f"All tasks built in {tasks_dir}")

    if args.write_job_config:
        job_path = task_root / "job.yaml"
        write_job_config(tasks_dir, job_path)
        print(f"Wrote job config -> {job_path}")

if __name__ == "__main__":
    main()
