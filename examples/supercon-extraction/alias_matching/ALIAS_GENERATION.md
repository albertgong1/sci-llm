# Property Alias Generation Methodology

This document outlines the exact algorithm used to generate the `property_aliases.json` file. The process relies on identifying systematic naming mismatches between the LLM predictions (False Positives) and Ground Truth (False Negatives) within the context of individual papers.

The code implementing this logic is located in `examples/extraction/alias_matching/analyze_mismatches.py`.

## 1. Input Data Processing
We iterate through the mismatch records generated by `examples/extraction/alias_matching/score_pr.py` (`out/score_pr_matches.csv`).
For each paper (identified by `refno`), we isolate:
*   **False Positives (FP)**: `mismatches[(mismatches["refno"] == refno) & (mismatches["status"] == "FP")]`
*   **False Negatives (FN)**: `mismatches[(mismatches["refno"] == refno) & (mismatches["status"] == "FN")]`

## 2. Pairwise Similarity Scoring
For every pair of unmatched properties $(P_{pred}, G_{gt})$ within the same paper, we calculate two metrics:

### A. Levenshtein Similarity Score
We use the `python-Levenshtein` C-extension for efficient string comparison.
**Code:**
```python
# Line 52
sim = ratio(str(fp_name).lower(), str(fn_name).lower())
```
This returns a float between 0.0 and 1.0 representing the character editing distance.

### B. Token Containment Check
We check if one property name is a complete substring of the other (case-insensitive). This catches cases where the LLM is more concise (e.g., "Hc2" vs "Upper critical field Hc2").
**Code:**
```python
# Line 55
contained = str(fp_name).lower() in str(fn_name).lower() or \
            str(fn_name).lower() in str(fp_name).lower()
```

### C. Top-K Selection per Paper
Instead of a hard similarity threshold, we employ a **Top-K ranking strategy** (with $K=3$) to capture the most likely intended targets, even if the similarity score is low.

**Logic:**
For every unmatched predicted property (FP) in a paper:
1.  We calculate the similarity (Levenshtein) against **ALL** missing Ground Truth properties (FNs) in that paper.
2.  We sort these potential matches by similarity score (descending).
3.  We select the **Top 3** matches and add them to the candidate pool.

This ensures we don't miss valid aliases that might have low string similarity (e.g., "Tc" vs "Critical Temperature" might have a moderate score, but it will likely be the #1 match in its paper if no other temperature fields are missing).

## 3. Global Aggregation and Selection
We aggregate these Top-3 candidates across the entire dataset of 15 papers.

### A. Aggregation
We count the frequencies of each unique $(P_{pred}, G_{gt})$ pair.
**Code:**
```python
# Line 72-75
summary = sugg_df.groupby(["pred_name", "gt_name"]).agg(
    count=("refno", "count"),
    avg_sim=("similarity", "mean")
).reset_index().sort_values("count", ascending=False)
```

### B. Top-K Cutoff and JSON Generation
To create the final map, we take the top 200 most frequent candidates.
**Code:**
```python
# Line 93
for _, row in summary.head(200).iterrows():
    # ...
    if p not in aliases:
        aliases[p] = g
```

## 4. Output
The resulting dictionary is saved to `property_aliases.json`.
*   **Keys**: The property name extracted by the LLM.
*   **Values**: The canonical property name in the Ground Truth dataset.

This file is then loaded by `score_pr.py` to normalize predictions before scoring.

## 5. Generated Artifacts
The scoring pipeline (`examples/extraction/alias_matching/score_pr.py` and `examples/extraction/alias_matching/analyze_mismatches.py`) produces several CSV artifacts in the output directory (`out/`) to facilitate debugging and analysis:

| File | Description |
| :--- | :--- |
| **`score_pr_matches.csv`** | **The Raw Evidence.** Contains every individual prediction-to-GT comparison. Includes columns for `refno`, `property_name`, `status` (TP/FP/FN), `pred_value`, and `gt_value`. Useful for row-level debugging. |
| **`score_pr_summary.csv`** | **Per-Property Metrics.** Aggregated Precision, Recall, and F1 scores for each property type (e.g. "Tc", "Pressure"). Used to generate the `report.txt` table. |
| **`score_pr_detailed.csv`** | **Per-Paper/Property Counts.** Contains the raw counts of TP/FP/FN for each (Paper, Property) tuple. Intermediate file used for aggregation. |
| **`gt_dump.csv`** | **Flattened Ground Truth.** A clean CSV dump of the Ground Truth dataset (e.g. from Hugging Face) as seen by the scorer. Useful to verify what the scorer considers "truth". |
| **`alias_candidates_review.csv`** | **For Expert Review.** The full list of Top 200 candidate pairs generated by `analyze_mismatches.py`. Unlike the JSON map, this CSV includes "clashes" (where one prediction maps to multiple GT targets), allowing an expert to choose the best one. |

## 6. Encompassment Analysis (Name Recall)
To differentiate between "Model missed the concept" and "Model found the concept but got the value wrong", we performed an intersection analysis on the `score_pr_detailed.csv` results.

### Definitions
*   **Misses (Total FN)**: properties that exist in the Ground Truth but were not scored as a perfect True Positive (Status=`FN > 0`).
*   **Name Matches (Encompassment)**: A subset of Misses where the model *did* extract a property with the **exact same name** as the missing GT property (Status=`FN > 0` AND `FP > 0` for the same Property Name). This implies the "concept" was found, but the value or unit mismatch caused a scoring failure.

### Current Baseline Results (v2.0.1)
*   **Total Misses (FN)**: 458
*   **Name Matches**: 128
*   **Encompassment Rate**: **27.9%**

**Interpretation**:
Approximately **28%** of the seemingly "missed" properties were actually found by the model (correct name), but failed the value verification step. The remaining **72%** are genuine misses or naming mismatches that require alias resolution.
# Pipeline Stages & Scripts

The alias generation and scoring pipeline consists of three main scripts located in `examples/extraction/alias_matching/`.

## 1. `score_pr.py` (Scoring & Analysis)
Calculates Precision, Recall, and F1 scores by comparing predictions against Ground Truth. It also generates the raw mismatch data needed for alias detection.

*   **Inputs**:
    *   `--preds`: Path to the predictions CSV (must contain `refno`, `property_name`, `value_string` columns).
    *   `--gt`: Path to Ground Truth CSV or Hugging Face dataset name (e.g., `albertgong1/sci-llm`).
    *   `--rubric`: Path to `examples/extraction/assets/rubric.csv`.
    *   `--clusters`: (Optional) Path to property clusters JSON.
*   **Outputs** (in `--output_dir`):
    *   `score_pr_matches.csv`: Detailed row-level comparison (used by `analyze_mismatches.py`).
    *   `score_pr_summary.csv`: Aggregated metrics.
    *   `report.txt`: Human-readable summary.

## 2. `analyze_mismatches.py` (Candidate Generation)
Analyzes the `score_pr_matches.csv` file to identify systematic naming mismatches and generate candidate aliases.

*   **Inputs**:
    *   `matches_file`: Path to `score_pr_matches.csv` (generated by `score_pr.py`).
*   **Outputs** (in the same directory as input):
    *   `alias_candidates_review.csv`: Top 200 candidate pairs for expert or LLM review.
    *   `property_aliases.json`: A draft JSON mapping of aliases (auto-generated from heuristics).

## 3. `verify_aliases.py` (LLM Verification)
Uses an LLM significantly verify the candidate pairs generated by the mismatch analyzer.

*   **Inputs**:
    *   `candidates_csv`: Path to `alias_candidates_review.csv`.
*   **Outputs** (in `--output_dir`, default `out_pr/`):
    *   `alias_verification_all__{model}.csv`: All verification results (including non-matches).
    *   `alias_verification_matches__{model}.csv`: Validated pairs confirmed by the LLM.
