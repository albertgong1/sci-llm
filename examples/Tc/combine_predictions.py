#!/usr/bin/env python3
"""Combine Edison prediction JSON files into a single CSV.

This script reads all JSON files generated by query_materials_with_edison.py
and combines them into a single pandas DataFrame and CSV file.
"""

import argparse
import json
import re
from pathlib import Path
from typing import List, Dict, Any

import pandas as pd


def extract_batch_number(filename: str) -> int:
    """Extract batch number from filename.
    
    Args:
        filename: Filename string (e.g., 'edison_precedent_batch=1__bs=10.json')
        
    Returns:
        Batch number as integer, or 0 if not found
    """
    match = re.search(r'batch=(\d+)', filename)
    if match:
        return int(match.group(1))
    return 0


def find_prediction_files(directory: Path, pattern: str = "edison_precedent_*.json") -> List[Path]:
    """Find all prediction JSON files in the directory.
    
    Args:
        directory: Directory to search
        pattern: Glob pattern to match files
        
    Returns:
        List of Path objects for matching files, sorted by batch number
    """
    files = list(directory.glob(pattern))
    # Sort by batch number extracted from filename
    files_sorted = sorted(files, key=lambda f: extract_batch_number(f.name))
    return files_sorted


def load_json_file(file_path: Path) -> List[Dict[str, Any]]:
    """Load a single JSON file.
    
    Args:
        file_path: Path to JSON file
        
    Returns:
        List of prediction dictionaries
    """
    with open(file_path, 'r') as f:
        data = json.load(f)
    return data


def flatten_full_response(record: Dict[str, Any]) -> Dict[str, Any]:
    """Flatten the full_response field into separate columns.
    
    Args:
        record: Single prediction record
        
    Returns:
        Flattened record with full_response fields at top level
    """
    flattened = record.copy()
    
    # Extract key fields from full_response if it exists
    if 'full_response' in record and record['full_response'] is not None:
        full_resp = record['full_response']
        
        # Extract useful fields
        for key in ['job_name', 'created_at', 'total_cost', 'total_queries', 'share_status']:
            if key in full_resp:
                flattened[f'response_{key}'] = full_resp[key]
        
        # Remove the full nested object (too verbose for CSV)
        del flattened['full_response']
    
    return flattened


def combine_predictions(
    input_dir: Path,
    pattern: str = "edison_precedent_*.json",
    flatten: bool = True,
) -> pd.DataFrame:
    """Combine all prediction JSON files into a single DataFrame.
    
    Args:
        input_dir: Directory containing JSON files
        pattern: Glob pattern to match files
        flatten: Whether to flatten the full_response field
        
    Returns:
        Combined DataFrame with all predictions
    """
    # Find all prediction files
    files = find_prediction_files(input_dir, pattern)
    
    if not files:
        raise ValueError(f"No files matching pattern '{pattern}' found in {input_dir}")
    
    print(f"Found {len(files)} prediction files:")
    for f in files:
        print(f"  - {f.name}")
    
    # Load and combine all files
    all_records = []
    for file_path in files:
        print(f"\nLoading {file_path.name}...")
        records = load_json_file(file_path)
        print(f"  Loaded {len(records)} predictions")
        
        # Extract batch number from filename
        batch_num = extract_batch_number(file_path.name)
        
        if flatten:
            records = [flatten_full_response(r) for r in records]
        
        # Add batch number to each record
        for record in records:
            record['batch_number'] = batch_num
        
        all_records.extend(records)
    
    # Create DataFrame
    df = pd.DataFrame(all_records)
    
    # Reorder columns to put batch_number near the beginning
    if 'batch_number' in df.columns:
        # Get all columns
        cols = df.columns.tolist()
        # Remove batch_number
        cols.remove('batch_number')
        # Put batch_number after icsd_id and reduced_formula if they exist
        insert_pos = 0
        if 'icsd_id' in cols:
            insert_pos = cols.index('icsd_id') + 1
        if 'reduced_formula' in cols and 'reduced_formula' in cols[:insert_pos+1]:
            insert_pos = cols.index('reduced_formula') + 1
        # Insert batch_number at appropriate position
        cols.insert(insert_pos, 'batch_number')
        # Reorder dataframe
        df = df.reindex(columns=cols)
    
    print(f"\n✓ Combined {len(df)} predictions from {len(files)} files")
    print(f"  Columns: {', '.join(df.columns)}")
    
    return df


def main(args: argparse.Namespace) -> None:
    """Main function.
    
    Args:
        args: Command line arguments
    """
    # Combine predictions
    try:
        df = combine_predictions(
            args.input_dir,
            pattern=args.pattern,
            flatten=not args.keep_nested,
        )
    except ValueError as e:
        print(f"Error: {e}")
        return
    except Exception as e:
        print(f"Error loading predictions: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # Save to CSV
    try:
        args.output.parent.mkdir(parents=True, exist_ok=True)
        df.to_csv(args.output, index=False)
        print(f"\n✓ Saved combined predictions to: {args.output}")
        print(f"  Shape: {df.shape[0]} rows × {df.shape[1]} columns")
    except Exception as e:
        print(f"Error saving CSV: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # Print summary statistics
    print("\n" + "=" * 60)
    print("Summary Statistics")
    print("=" * 60)
    
    if 'status' in df.columns:
        print("\nStatus distribution:")
        print(df['status'].value_counts())
    
    if 'has_successful_answer' in df.columns:
        print("\nSuccessful answers:")
        print(df['has_successful_answer'].value_counts())
    
    if 'reduced_formula' in df.columns:
        print(f"\nUnique materials: {df['reduced_formula'].nunique()}")
    
    # Show sample of data
    print("\n" + "=" * 60)
    print("Sample of combined data (first 3 rows):")
    print("=" * 60)
    
    # Display key columns only
    display_cols = ['icsd_id', 'reduced_formula', 'has_successful_answer', 'status']
    display_cols = [col for col in display_cols if col in df.columns]
    
    if display_cols:
        print(df[display_cols].head(3).to_string(index=False))


def get_parser() -> argparse.ArgumentParser:
    """Get argument parser."""
    parser = argparse.ArgumentParser(
        description="Combine Edison prediction JSON files into a single CSV"
    )
    
    parser.add_argument(
        "--input-dir",
        "-i",
        type=Path,
        default=Path(__file__).parent / "out",
        help="Directory containing JSON prediction files (default: out)",
    )
    
    parser.add_argument(
        "--output",
        "-o",
        type=Path,
        default=Path(__file__).parent / "combined_predictions.csv",
        help="Output CSV file path (default: combined_predictions.csv)",
    )
    
    parser.add_argument(
        "--pattern",
        "-p",
        type=str,
        default="edison_precedent_*.json",
        help="Glob pattern to match JSON files (default: edison_precedent_*.json)",
    )
    
    parser.add_argument(
        "--keep-nested",
        action="store_true",
        help="Keep the nested full_response field (makes CSV very large)",
    )
    
    return parser


if __name__ == "__main__":
    parser = get_parser()
    args = parser.parse_args()
    main(args)

