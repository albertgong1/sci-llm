"""Filter and classify property candidates from extracted properties CSV files.

This script processes CSV files generated by mass_extract_properties_from_llm.py
from the unsupervised_llm_extraction directory and adds a "data_type" column that
classifies the value_string as:

- "numerical": Single numeric values (with or without units)
  Examples: "25.1 mPa·S", "~31.6 mN·m-1", "around 12 mN·m-1"

- "numerical_range": Numeric ranges or uncertainties
  Examples: "1.54±0.04 to 2.43±0.18", "2.06 ± 0.08", "> 2", "5 ~ 10"

- "string": Non-numeric text descriptions
  Examples: "good emulsifying ability", "rapidly adsorbing"

The script reads CSV files from: <output_dir>/unsupervised_llm_extraction/
And writes a single combined CSV file to: <output_dir>/candidates/extracted_properties_combined.csv

Usage (from within an examples subdirectory):
```bash
# Example: from examples/biosurfactants-extraction/
cd examples/biosurfactants-extraction

# Process with specific output directory
uv run pbench-filter -od OUTPUT_DIR
```
"""

import argparse
import logging
import re
from pathlib import Path

import pandas as pd

import pbench

logger = logging.getLogger(__name__)


def is_numeric(value_str: str) -> bool:
    """Check if a string represents a numeric value.

    Args:
        value_str: String to check

    Returns:
        True if the string is numeric, False otherwise

    """
    if not value_str or not isinstance(value_str, str):
        return False

    # Clean the string - remove common units and special characters
    # but keep numbers, decimal points, signs, and scientific notation
    cleaned = re.sub(r"[^\d\.\-\+eE]", "", value_str.strip())

    if not cleaned:
        return False

    try:
        float(cleaned)
        return True
    except ValueError:
        return False


def classify_value_string(value_str: str) -> str:
    """Classify a value_string as string, numerical, or numerical_range.

    Uses the logic from src/pbench/utils.py to detect ranges.

    Args:
        value_str: The value string to classify

    Returns:
        One of: "string", "numerical", "numerical_range"

    """
    if pd.isna(value_str) or not value_str or not isinstance(value_str, str):
        return "string"

    value_str = str(value_str).strip()

    # Check for range indicators (from utils.py lines 33 and 40-43)
    # More specific patterns to avoid false positives

    # Check for explicit range patterns with numbers on both sides
    # Pattern: number [range_indicator] number
    range_patterns = [
        r"\d+\.?\d*\s*[–~]\s*\d+\.?\d*",  # e.g., "1.5–2.5" (en-dash), "5 ~ 10" (tilde)
        r"\d+\.?\d*\s+to\s+\d+\.?\d*",  # e.g., "1.5 to 2.5"
        r">\s*\d+\.?\d*",  # e.g., "> 2"
        r"<\s*\d+\.?\d*",  # e.g., "< 2"
        r"\d+\.?\d*\s*±\s*\d+\.?\d*",  # e.g., "1.5±0.1" (plus-minus with two numbers)
        r"from\s+.*\s+to\s+.*\d",  # e.g., "from > 2 to ~1.7"
    ]

    for pattern in range_patterns:
        if re.search(pattern, value_str, re.IGNORECASE):
            return "numerical_range"

    # Check if it's a pure number
    if is_numeric(value_str):
        return "numerical"

    # Check if it contains numbers (might be a number with units)
    # If it has digits and looks like a measurement, classify as numerical
    if re.search(r"\d", value_str):
        # Look for patterns like "~31.6", "around 12", "approximately 5.5"
        # These are approximate single values, not ranges
        approx_pattern = r"(?:~|around|approximately|about|ca\.?)\s*\d+\.?\d*"
        if re.search(approx_pattern, value_str, re.IGNORECASE):
            return "numerical"

        # Extract just the numeric part at the beginning
        numeric_part = re.match(r"^\s*[-+]?[\d]+\.?[\d]*(?:[eE][-+]?\d+)?", value_str)
        if numeric_part:
            # It starts with a number, likely a measurement with units
            return "numerical"

    return "string"


def process_csv_file(input_path: Path) -> pd.DataFrame | None:
    """Process a single CSV file and add data_type column.

    Args:
        input_path: Path to input CSV file

    Returns:
        DataFrame with data_type column added, or None if processing failed

    """
    logger.info(f"Processing {input_path.name}")

    # Read CSV
    # refno on arxiv IDs can be coerced to float, we don't want that
    # e.g. 08123.1 should stay "08123.1", it should not become 8123.1
    df = pd.read_csv(input_path, dtype={"refno": "string"})

    # Check if value_string column exists
    if "value_string" not in df.columns:
        logger.warning(f"No 'value_string' column found in {input_path.name}, skipping")
        return None

    # Check if data_type column already exists
    if "data_type" in df.columns:
        logger.warning(
            f"'data_type' column already exists in {input_path.name}, will overwrite it"
        )

    # Add data_type column
    df["data_type"] = df["value_string"].apply(classify_value_string)

    # Log statistics
    type_counts = df["data_type"].value_counts()
    logger.info(f"Data type distribution: {type_counts.to_dict()}")

    return df


def main() -> None:
    """CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Filter and classify property candidates from extracted properties CSV files"
    )

    # Add base pbench arguments
    parser = pbench.add_base_args(parser)

    args = parser.parse_args()

    # Setup logging using pbench's setup_logging
    pbench.setup_logging(args.log_level)

    # Define input and output directories
    input_dir = args.output_dir / "unsupervised_llm_extraction"
    output_dir = args.output_dir / "candidates"

    # Check if input directory exists
    if not input_dir.exists():
        logger.error(f"Input directory does not exist: {input_dir}")
        logger.error(f"Expected to find CSV files in: {input_dir}")
        return

    # Find all CSV files in input directory
    csv_files = sorted(input_dir.glob("extracted_properties*.csv"))

    if not csv_files:
        logger.warning(
            f"No CSV files matching pattern 'extracted_properties*.csv' found in {input_dir}"
        )
        return

    logger.info(f"Found {len(csv_files)} CSV files to process")
    logger.info(f"Input directory: {input_dir}")
    logger.info(f"Output directory: {output_dir}")

    # Create output directory if it doesn't exist
    output_dir.mkdir(parents=True, exist_ok=True)

    # Process each CSV file and collect dataframes
    all_dfs = []
    for csv_file in csv_files:
        df = process_csv_file(csv_file)
        if df is not None:
            all_dfs.append(df)

    if not all_dfs:
        logger.error("No CSV files were successfully processed")
        return

    # Concatenate all dataframes
    combined_df = pd.concat(all_dfs, ignore_index=True)

    # Save to single CSV file
    output_file = output_dir / "extracted_properties_combined.csv"
    combined_df.to_csv(output_file, index=False)

    logger.info(f"Combined {len(all_dfs)} CSV files into {output_file}")
    logger.info(f"Total rows: {len(combined_df)}")

    # Log overall statistics
    type_counts = combined_df["data_type"].value_counts()
    logger.info(f"Overall data type distribution: {type_counts.to_dict()}")


if __name__ == "__main__":
    main()
